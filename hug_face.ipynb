{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ab03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, site\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"site-packages:\", site.getsitepackages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe5cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\ayish\\Lexicon Projects\\Hugging Face\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c268dfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayish\\Lexicon Projects\\Hugging Face\\hf311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, torch, transformers, huggingface_hub\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400d4482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "Executable: c:\\Users\\ayish\\Lexicon Projects\\Hugging Face\\hf311\\Scripts\\python.exe\n",
      "Torch: 2.8.0+cpu | CUDA available -> False\n",
      "Transformers: 4.56.1\n",
      "HF Hub: 0.35.0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available ->\", torch.cuda.is_available())\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"HF Hub:\", huggingface_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0482489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output: The committee is currently evaluating potential options for improvement.\n"
     ]
    }
   ],
   "source": [
    "gen = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=-1)\n",
    "out = gen(\n",
    "    \"Rewrite in simpler English: The committee is currently evaluating potential options for improvement.\",\n",
    "    do_sample=False,  top_p=1.0, max_new_tokens=32\n",
    ")[0][\"generated_text\"]\n",
    "print(\"Sample output:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ce4e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import pipeline\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee65080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Instruction Rewrite ---\n",
      "Input : The horse raced past the barn fell,\" where the initial interpretation is misleading. Other challenging sentences rely on wordplay, like \"Time flies like an arrow; fruit flies like a banana,\" or complex grammatical constructions and long clauses, as seen in sentences with multiple embedded phrases.\n",
      "Output: Output exactly one sentence, 8 to 24 words, ending with a period. Sentence: The horse raced past the barn fell,\" where the\n",
      "constraint not satisfied: output must end with a period.\n",
      "constraint not satisfied: output must be exactly one sentence (no '?' or '!' or multiple sentences).\n"
     ]
    }
   ],
   "source": [
    "rewrite_pipe= pipeline(\n",
    "    \"text2text-generation\", model=\"google/flan-t5-base\", device=-1\n",
    ")\n",
    "user_sentence = input(\"Enter one English sentence to simplify: \").strip()\n",
    "prompt = (\n",
    "    \"Rewrite the sentence in simpler English. \"                 \n",
    "    \"Output exactly one sentence, 8 to 24 words, ending with a period.\\n\\n\"  \n",
    "    f\"Sentence: {user_sentence}\"                                \n",
    ")\n",
    "result = rewrite_pipe(\n",
    "    prompt,\n",
    "    do_sample=False, top_p=1.0, max_new_tokens=32\n",
    ")\n",
    "\n",
    "generated = result[0][\"generated_text\"]\n",
    "\n",
    "generated_norm = \" \".join(generated.strip().split())\n",
    "\n",
    "word_list = re.findall(r\"\\b[\\w'-]+\\b\", generated_norm)\n",
    "word_count= len(word_list)\n",
    "#check1\n",
    "ends_with_period = generated_norm.endswith(\".\") \n",
    "#check2\n",
    "punct_enders = re.findall(r\"[.!?]\", generated_norm)\n",
    "one_sentence = (len(punct_enders) == 1) and ends_with_period \n",
    "#check3\n",
    "length_ok = (8 <= word_count <= 24)\n",
    "\n",
    "print(\"\\n--- Instruction Rewrite ---\")\n",
    "print(\"Input :\", user_sentence)         \n",
    "print(\"Output:\", generated_norm)\n",
    "\n",
    "if not ends_with_period:\n",
    "    print(\"constraint not satisfied: output must end with a period.\")\n",
    "if not one_sentence:\n",
    "    print(\"constraint not satisfied: output must be exactly one sentence (no '?' or '!' or multiple sentences).\")\n",
    "if not length_ok:\n",
    "    print(f\"constraint not satisfied: output must be 8â€“24 words (got {word_count}).\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eaf5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter three sentences for sentiment analysis:\n",
      "\n",
      "===  Sentiment (SST-2) ===\n",
      "1) The restaurant's food was top-notch, and the service was incredibly friendly! - POSITIVE  (score: 1.000)\n",
      "2) I was so disappointed by the unprofessional staff and the overall poor quality of the experience. - NEGATIVE  (score: 1.000)\n",
      "3) I adore this phone's camera, but its battery life is truly terrible - NEGATIVE  (score: 0.999)\n",
      "\n",
      "Note: SST-2 (the dataset used to fine-tune this model) is a binary sentiment task with only 'POSITIVE' and 'NEGATIVE' labels. The original Stanford Sentiment Treebank includes fine-grained annotations, but the SST-2 configuration collapses them into two classes, so there is no 'Neutral'. This means mixed/neutral statements may be forced into positive or negative with lower confidence. If you need a neutral option, use a model trained for 3+ classes (e.g., a 3-class sentiment model) or inspect the confidence score: values near 0.5 often indicate ambiguity.\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipe = pipeline(\n",
    "    \"text-classification\",                                   \n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "    device=-1                                                \n",
    ")\n",
    "print(\"\\nPlease enter three sentences for sentiment analysis:\") \n",
    "s1 = input(\"Sentence 1: \").strip()                              \n",
    "s2 = input(\"Sentence 2: \").strip()                              \n",
    "s3 = input(\"Sentence 3: \").strip()\n",
    "sentences = [s1, s2, s3] \n",
    "sentiment_results = sentiment_pipe(sentences, truncation=True)\n",
    "print(\"\\n===  Sentiment (SST-2) ===\")\n",
    "for i, (text, res) in enumerate(zip(sentences, sentiment_results), start=1):  \n",
    "    label = res[\"label\"]                                         \n",
    "    score = float(res[\"score\"])                                 \n",
    "    print(f\"{i}) {text} - {label}  (score: {score:.3f})\")  \n",
    "\n",
    "print(\n",
    "    \"\\nNote: SST-2 (the dataset used to fine-tune this model) is a binary sentiment task with only \"\n",
    "    \"'POSITIVE' and 'NEGATIVE' labels. The original Stanford Sentiment Treebank includes fine-grained \"\n",
    "    \"annotations, but the SST-2 configuration collapses them into two classes, so there is no 'Neutral'. \"\n",
    "    \"This means mixed/neutral statements may be forced into positive or negative with lower confidence. \"\n",
    "    \"If you need a neutral option, use a model trained for 3+ classes (e.g., a 3-class sentiment model) \"\n",
    "    \"or inspect the confidence score: values near 0.5 often indicate ambiguity.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871cb8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [ln.strip() for ln in f.readlines()]\n",
    "    return [ln for ln in lines if ln != \"\"]\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Step 1: Read inputs from a .txt file\")\n",
    "    parser.add_argument(\"--infile\", default=\"textfile.txt\", help=\"Path to input .txt (default: textfile.txt)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    inputs = read_lines(args.infile)\n",
    "    if not inputs:\n",
    "        print(\"No non-empty lines found in infile.\")\n",
    "        return\n",
    "    print(f\"Loaded {len(inputs)} lines from: {args.infile}\")\n",
    "    for idx, text in enumerate(inputs, start=1):\n",
    "        print(f\"{idx:02d}. {text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf68283",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"        The Eiffel Tower in Paris attracts millions of tourists every year.\"\n",
    "t = \" \".join(text.strip().split())  \n",
    "enders = sum(ch in \".!?\" for ch in t) \n",
    "for ch in t:\n",
    "    print(ch in \".!?\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383b259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayish\\Lexicon Projects\\Hugging Face\\hf311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([12, 48])\n",
      "Attention mask shape: torch.Size([12, 48])\n",
      "Padding token ID: 0\n",
      "attention_mask last row: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name= \"google/flan-t5-base\"\n",
    "\n",
    "max_new_tokens = 32\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "raw_prompts = [\n",
    "    # Rewrite (4)\n",
    "    \"Rewrite the sentence in simpler English. End with a period. Sentence: 'Pythonâ€™s clear syntax helps beginners focus on problem-solving.' Output:\",\n",
    "    \"Rewrite the sentence in simpler English. End with a period. Sentence: 'Version control lets teams track changes and work safely together.' Output:\",\n",
    "    \"Rewrite the sentence in simpler English. End with a period. Sentence: 'Preprocessing text often includes lowercasing and removing extra spaces.' Output:\",\n",
    "    \"Rewrite the sentence in simpler English. End with a period. Sentence: 'Short prompts run faster on CPU because attention scales with length.' Output:\",\n",
    "    # Explain (4)\n",
    "    \"Explain in one sentence what a learning rate does. End with a period.\",\n",
    "    \"Explain in one sentence what an API key is used for. End with a period.\",\n",
    "    \"Explain in one sentence what a unit test checks. End with a period.\",\n",
    "    \"Explain in one sentence what a tokenizer does in NLP. End with a period.\",\n",
    "    # Summarize (4)\n",
    "    \"Summarize in one sentence: 'Pipelines bundle tokenization, the model, and decoding. They are great for quick demos on CPU.' Output:\",\n",
    "    \"Summarize in one sentence: 'Batching several prompts can improve throughput. Padding and masks keep shapes compatible.' Output:\",\n",
    "    \"Summarize in one sentence: 'Beam search is deterministic and often fluent. Sampling adds creativity but may drift.' Output:\",\n",
    "    \"Summarize in one sentence: 'SentencePiece and WordPiece split text into subwords. This keeps vocabulary small and improves coverage.' Output:\",\n",
    "]\n",
    "\n",
    "batch_prompts = [f\"Respond in one sentence: {p}\" for p in raw_prompts]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "enc_batch = tokenizer(\n",
    "    batch_prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Input IDs shape:\", enc_batch[\"input_ids\"].shape)  # [B, L]\n",
    "print(\"Attention mask shape:\", enc_batch[\"attention_mask\"].shape)  # [B, L]\n",
    "print(\"Padding token ID:\", tokenizer.pad_token_id)\n",
    "\n",
    "print(\"attention_mask last row:\", enc_batch[\"attention_mask\"][-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28810825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generated outputs (per prompt) ===\n",
      "01. [Greedy] Pythonâ€™s clear syntax helps beginners focus on problem-solving.\n",
      "    [Beam]   Pythonâ€™s clear syntax helps beginners focus on problem-solving.\n",
      "    [Sample] Pythonâ€™s clear syntax helps beginners focus on problem-solving.\n",
      "02. [Greedy] Version control lets teams track changes and work safely together.\n",
      "    [Beam]   Version control lets teams track changes and work safely together.\n",
      "    [Sample] Version control lets teams track changes and work safely together.\n",
      "03. [Greedy] Preprocessing text often includes lowercasing and removing extra spaces.\n",
      "    [Beam]   Preprocessing text often includes lowercasing and removing extraspaces.\n",
      "    [Sample] Preprocessing text often includes lowercasing and removing extra spaces.\n",
      "04. [Greedy] Short prompts run faster on CPU because attention scales with length.\n",
      "    [Beam]   Short prompts run faster on CPU because attention scales with length.\n",
      "    [Sample] Short prompts run faster on CPU because attention scales with length.\n",
      "05. [Greedy] A student's rate of learning is the rate at which a student's brain learns a new skill.\n",
      "    [Beam]   A student's learning rate is the rate at which he or she learns a new skill.\n",
      "    [Sample] a student needs to be on time.\n",
      "06. [Greedy] API key is used to access the API key.\n",
      "    [Beam]   An API key is used to connect to a database.\n",
      "    [Sample] API key is used to create a key for an application.\n",
      "07. [Greedy] unit test is a test to measure the strength of a chemical.\n",
      "    [Beam]   A unit test is a test of a unit of measurement.\n",
      "    [Sample] Unit testing is a test of a quantity of material.\n",
      "08. [Greedy] A tokenizer is a digitized representation of a number.\n",
      "    [Beam]   A tokenizer is a type of tokenizer.\n",
      "    [Sample] A tokenizer is a piece of software that reads a string of random numbers.\n",
      "09. [Greedy] Pipelines are a great way to get your code to work.\n",
      "    [Beam]   Pipelines are great for quick demos on CPU.\n",
      "    [Sample] Several open-source projects are already evaluating the use of pipelines on a computer.\n",
      "10. [Greedy] A new way to make a spooky spooky mask is to use a mask.\n",
      "    [Beam]   You can use a variety of templates to create a simple, easy-to-use template. Choose a template that is easy to use and easy\n",
      "    [Sample] You can also write your own notes for a class.\n",
      "11. [Greedy] The 'beam search' is a method of determining the direction of a beam.\n",
      "    [Beam]   Beam search is deterministic and often fluent. Sampling adds creativity but may drift.\n",
      "    [Sample] The best method to explore the depths of the night sky is to take a long walk to the top of the trees.\n",
      "12. [Greedy] WordPiece and WordPiece are both free and open source.\n",
      "    [Beam]   WordPiece and SentencePiece have been renamed WordPiece and SentencePiece\n",
      "    [Sample] WordPiece has added a new keyboard and interface.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_grad_enabled(False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "input_ids = enc_batch.input_ids           # shape: (batch, seq_len)\n",
    "attention_mask = enc_batch.attention_mask # 1 for real tokens, 0 for padding\n",
    "\n",
    "batch_input_ids = input_ids.to(device)\n",
    "batch_attention_mask = attention_mask.to(device)\n",
    "\n",
    "greedy_outputs= model.generate(\n",
    "    input_ids= batch_input_ids,\n",
    "    attention_mask= batch_attention_mask,\n",
    "    max_new_tokens= 32,\n",
    "    do_sample= False,\n",
    "    num_beams=1\n",
    ")\n",
    "\n",
    "beam_outputs = model.generate(\n",
    "    input_ids= batch_input_ids,\n",
    "    attention_mask= batch_attention_mask,\n",
    "    max_new_tokens= 32,\n",
    "    do_sample= False,\n",
    "    num_beams= 5    \n",
    ")\n",
    "torch.manual_seed(42)\n",
    "sampling_outputs = model.generate(\n",
    "    input_ids= batch_input_ids,\n",
    "    attention_mask= batch_attention_mask,\n",
    "    max_new_tokens= 32,\n",
    "    do_sample= True,\n",
    "    temperature= 0.8,\n",
    "    top_p= 0.9\n",
    ")\n",
    "greedy_texts = [s.strip() for s in tokenizer.batch_decode(greedy_outputs, skip_special_tokens=True)]\n",
    "beam_texts   = [s.strip() for s in tokenizer.batch_decode(beam_outputs,   skip_special_tokens=True)]\n",
    "sample_texts = [s.strip() for s in tokenizer.batch_decode(sampling_outputs, skip_special_tokens=True)]\n",
    "\n",
    "print(\"\\n=== Generated outputs (per prompt) ===\")\n",
    "for i, (g, b, s) in enumerate(zip(greedy_texts, beam_texts, sample_texts), start=1):\n",
    "    print(f\"{i:02d}. [Greedy] {g}\")\n",
    "    print(f\"    [Beam]   {b}\")\n",
    "    print(f\"    [Sample] {s}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc45ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Automatic checks â€” summary per strategy ===\n",
      "Strategy    Pass%  AvgWC  StdWC  Repeat%\n",
      "Greedy      100.0   11.0    2.5      8.3\n",
      "Beam         66.7   11.2    4.3     16.7\n",
      "Sampling     91.7   11.2    4.1      8.3\n",
      "\n",
      "-- Failure reasons for Greedy --\n",
      "05: repetition detected | A student's rate of learning is the rate at which a student's brain learns a new skill.\n",
      "\n",
      "-- Failure reasons for Beam --\n",
      "07: repetition detected | A unit test is a test of a unit of measurement.\n",
      "08: word count 7 outside 8â€“24 | A tokenizer is a type of tokenizer.\n",
      "10: missing final period | You can use a variety of templates to create a simple, easy-to-use template. Choose a template that is easy to use and easy\n",
      "11: more than one sentence | Beam search is deterministic and often fluent. Sampling adds creativity but may drift.\n",
      "12: more than one sentence, missing final period, repetition detected | WordPiece and SentencePiece have been renamed WordPiece and SentencePiece\n",
      "\n",
      "-- Failure reasons for Sampling --\n",
      "05: word count 7 outside 8â€“24 | a student needs to be on time.\n",
      "11: repetition detected | The best method to explore the depths of the night sky is to take a long walk to the top of the trees.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def is_one_sentence(text):\n",
    "    groups= re.findall(r'[.!?]+', text)\n",
    "    return len(groups)==1\n",
    "\n",
    "def ends_with_period(text):\n",
    "    return text.strip().endswith(\".\")\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.strip().split())\n",
    "\n",
    "def in_word_window(n, low=8, high=24):\n",
    "    return low <= n <= high\n",
    "\n",
    "def has_repetition(text):\n",
    "    toks = text.lower().split()\n",
    "    bigrams = list(zip(toks, toks[1:]))\n",
    "    seen = set()\n",
    "    for bg in bigrams:\n",
    "        if bg in seen:\n",
    "            return True\n",
    "        seen.add(bg)\n",
    "    return False\n",
    "def evaluate(outputs):\n",
    "    \n",
    "    n = len(outputs)\n",
    "    wcs = [word_count(t) for t in outputs]\n",
    "    passes = [\n",
    "        is_one_sentence(t) and ends_with_period(t) and in_word_window(word_count(t))\n",
    "        for t in outputs\n",
    "    ]\n",
    "    reps = [has_repetition(t) for t in outputs]\n",
    "\n",
    "    pass_rate = 100.0 * sum(passes) / n if n else 0.0\n",
    "    avg_wc = sum(wcs) / n if n else 0.0\n",
    "    std_wc = math.sqrt(sum((w - avg_wc) ** 2 for w in wcs) / n) if n else 0.0\n",
    "    rep_pct = 100.0 * sum(reps) / n if n else 0.0\n",
    "\n",
    "    return pass_rate, avg_wc, std_wc, rep_pct\n",
    "\n",
    "\n",
    "results = {\n",
    "    \"Greedy\":   evaluate(greedy_texts),\n",
    "    \"Beam\":     evaluate(beam_texts),\n",
    "    \"Sampling\": evaluate(sample_texts),\n",
    "}\n",
    "print(\"\\n=== Automatic checks â€” summary per strategy ===\")\n",
    "print(f\"{'Strategy':<10} {'Pass%':>6} {'AvgWC':>6} {'StdWC':>6} {'Repeat%':>8}\")\n",
    "for name, (pass_rate, avg_wc, std_wc, rep_pct) in results.items():\n",
    "    print(f\"{name:<10} {pass_rate:6.1f} {avg_wc:6.1f} {std_wc:6.1f} {rep_pct:8.1f}\")\n",
    "\n",
    "def failure_reason(t):\n",
    "    \"\"\"Return a concise string explaining why a line failed.\"\"\"\n",
    "    reasons = []\n",
    "    if not is_one_sentence(t):\n",
    "        reasons.append(\"more than one sentence\")\n",
    "    if not ends_with_period(t):\n",
    "        reasons.append(\"missing final period\")\n",
    "    wc = word_count(t)\n",
    "    if not in_word_window(wc):\n",
    "        reasons.append(f\"word count {wc} outside 8â€“24\")\n",
    "    if has_repetition(t):\n",
    "        reasons.append(\"repetition detected\")\n",
    "    return \", \".join(reasons) or \"passes\"\n",
    "\n",
    "for name, texts in [(\"Greedy\", greedy_texts), (\"Beam\", beam_texts), (\"Sampling\", sample_texts)]:\n",
    "    any_fail = False\n",
    "    for i, t in enumerate(texts, 1):\n",
    "        r = failure_reason(t)\n",
    "        if r != \"passes\":\n",
    "            if not any_fail:\n",
    "                print(f\"\\n-- Failure reasons for {name} --\")\n",
    "                any_fail = True\n",
    "            print(f\"{i:02d}: {r} | {t}\")\n",
    "    if not any_fail:\n",
    "        print(f\"\\n-- Failure reasons for {name} --\\n(all passed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b45c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Timing ===\n",
      "Single input: ~0.192s\n",
      "Small batch : ~2.164s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "_ = model.generate(**tokenizer(\"warmup\", return_tensors=\"pt\").to(device),\n",
    "                   do_sample=False, num_beams=1, max_new_tokens=32)\n",
    "\n",
    "rep_prompt = batch_prompts[0]  # any representative prompt\n",
    "t0 = time.perf_counter()\n",
    "enc1 = tokenizer([rep_prompt], return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "_ = model.generate(input_ids=enc1.input_ids.to(device),\n",
    "                   attention_mask=enc1.attention_mask.to(device),\n",
    "                   do_sample=False, num_beams=1,  # greedy for consistency\n",
    "                   max_new_tokens=32)\n",
    "t1 = time.perf_counter()\n",
    "single_sec = t1 - t0\n",
    "\n",
    "# -------- 2) Small-batch timing (12 prompts) --------\n",
    "t2 = time.perf_counter()\n",
    "encb = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "_ = model.generate(input_ids=encb.input_ids.to(device),\n",
    "                   attention_mask=encb.attention_mask.to(device),\n",
    "                   do_sample=False, num_beams=1,  # greedy\n",
    "                   max_new_tokens=32)\n",
    "t3 = time.perf_counter()\n",
    "batch_sec = t3 - t2\n",
    "\n",
    "print(\"\\n=== Timing ===\")\n",
    "print(f\"Single input: ~{single_sec:.3f}s\")\n",
    "print(f\"Small batch : ~{batch_sec:.3f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
